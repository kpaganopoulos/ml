{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupe Assignment 3\n",
    "## Team 7\n",
    "### 04/02/2020\n",
    "\n",
    "#### Let us load the data set on spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"D:\\Johanna\\Ecole\\Imperial\\Machine learning\\smsspamcollection\\SMSSpamCollection\", header=None, sep='\\t', lineterminator='\\n', names=['type', 'text'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To create our na√Øve Bayes classifier, we will first remove all the punctuation and all the numbers from the text which will be in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "for p in string.punctuation :\n",
    "    data['text'] = data['text'].str.replace(p, '')\n",
    "data['text'] = data['text'].str.replace('\\d+', '')\n",
    "data['text'] = data['text'].str.lower()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 2) (1000, 2) (2072, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data.iloc[:,:-1]\n",
    "X = data.iloc[:, -1]\n",
    "\n",
    "X_intermediary, X_test = train_test_split(data, test_size=2072/5572, random_state=42)\n",
    "X_train, X_validate = train_test_split(X_intermediary, test_size=1000/3500, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_validate.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us consider the following class which defines a Naive Bayes Classifier for spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesForSpam :\n",
    "    \n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set(' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages))/ (len(hamMessages)+len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m]))/len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m]))/len(spamMessages)\n",
    "            self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "        \n",
    "    def train2(self, hamMessages, spamMessages):\n",
    "        self.words = set(' '.join(hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages))/ (len(hamMessages)+len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m]))/len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m]))/len(spamMessages)\n",
    "            if prob1*20<prob2:\n",
    "                self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "                spamkeywords.append(w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "    \n",
    "    def predict(self, message):\n",
    "        posteriors = np.copy(self.priors)\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w in message.lower():\n",
    "                posteriors *= self.likelihoods[:,i]\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:, i]\n",
    "            posteriors = posteriors/np.linalg.norm(posteriors, ord = 1)\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]\n",
    "    \n",
    "    def score(self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape(2,2)\n",
    "        for m, l in zip(messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] +=1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] +=1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] +=1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] +=1\n",
    "        return (confusion[0,0] + confusion[1,1])/float(confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This class is composed of 4 functions.\n",
    "#### 1. The first function train\n",
    "\n",
    "This function is taking as arguments 2 lists : the ham messages one and the spam messages one. It changes the classifier attributes :\n",
    "\n",
    "- $words$ is the set of each word which appears at least one in one message (either spam or ham). As every set, each word is contained only once.\n",
    "\n",
    "- $priors$ is the list of the 2 prior probabilities as defined in Bayes Theorem. $prior[0]$ is the probability of being a ham and $prior[1]$ is the probability of being the spam. It is simply computed as the proportion of ham or spam messages into the whole set of messages.\n",
    "\n",
    "- $likelihoods[:, 0]$ is the list of the likelihood as defined in Bayes Theorem for ham messages, in other words, each row is the probability a given word appears knowing that the message is a ham message. It is computed as the proportion of ham messages in which this word appears. Actually, it is capped to 0.95.\n",
    "\n",
    "- $likelihoods[:, 1]$ is the list of the likelihood as defined in Bayes Theorem for spam messages, in other words, each row is the probability a given word appears knowing that the message is a spam message. It is computed as the proportion of spam messages in which this word appears. Actually, it is capped to 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The second function train\n",
    "\n",
    "This function is taking as arguments 2 lists : the ham messages one and the spam messages one. It changes the classifier attributes :\n",
    "\n",
    "- $words$ is first the set of each word which appears at least one in one message (either spam or ham). As every set, each word is contained only once. At the end, it is replaced by the set of word where the proportion in spam messages is at least 20 times superior to the proportion in ham messages.\n",
    "\n",
    "- $priors$ is the same as before.\n",
    "\n",
    "- $likelihoods$ is the same as before but only for spam key words (set of word where the proportion in spam messages is at least 20 times superior to the proportion in ham messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The thrid function: predict\n",
    "\n",
    "This function takes as argument a new message and returns the predicted category as well as the posterior probability $P(X|M)$ defines as $P(X)\\times P(M|X)$ where $X$ is the predicted category.\n",
    "\n",
    "To calculate the posterior probabilities, this function takes the prior probabilities from the priors attribute of the classifier and computes the likelihood as the product of all the likelihoods of each word which is in both the message and the words attribute, according to the conditional independence. It is using the Bayes Theorem. The predicted category is the one with the higher posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. The fourth function: score\n",
    "\n",
    "This function is used on a trained classifier, taking as arguments a list of messages and their category. It returns the accuracy and the confusion matrix. Indeed, for each message, it computes the predicted category thanks to the predict function and compares it to the category from labels. If we suppose that ham is the positive label, $confusion[0,0]$ is the number of true positive. $confusion[0,1]$ is the number of false positive. $confusion[1,0]$ is the number of false negative and $confusion[0,0]$ is the number of true negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us now train 2 different classifiers with our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamMessages_train = X_train[X_train['type'] == 'ham'][\"text\"].tolist()\n",
    "spamMessages_train = X_train[X_train['type'] == 'spam'][\"text\"].tolist()\n",
    "\n",
    "\n",
    "cfl1 = NaiveBayesForSpam()\n",
    "cfl2 = NaiveBayesForSpam()\n",
    "\n",
    "\n",
    "cfl2.train2(hamMessages_train, spamMessages_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us have a look into the performance of those 2 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.963, array([[845.,  24.],\n",
      "       [ 13., 118.]]))\n",
      "--- 87.3810453414917 seconds ---\n"
     ]
    }
   ],
   "source": [
    "messages_validate = X_validate[\"text\"].tolist()\n",
    "labels_validate = X_validate[\"type\"].tolist()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "s1 = cfl1.score(messages_validate, labels_validate)\n",
    "print(s1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.953, array([[852.,  41.],\n",
      "       [  6., 101.]]))\n",
      "--- 3.6581149101257324 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "s2 = cfl2.score(messages_validate, labels_validate)\n",
    "print(s2)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the second training is slightly less performant since it has an accuracy lower of 1%. Yet, it is more than 20 times faster. Indeed, the set of words is significantly reduced, increasing the execution speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of both classifier on the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974 0.9728\n"
     ]
    }
   ],
   "source": [
    "messages_train = X_train[\"text\"].tolist()\n",
    "labels_train = X_train[\"type\"].tolist()\n",
    "s1 = cfl1.score(messages_train, labels_validate)\n",
    "s2 = cfl2.score(messages_train, labels_validate)\n",
    "\n",
    "print(s1[0], s2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that accuracy is lower for both training and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation set, we have 13 messages classified as spam but were actually ham. \n",
    "To reduce this number, we could create the reverse of spam key words : ham key words. This list would contain all the words where the proportion in ham messages is at least 20 times superior to the proportion in spam messages. It could increase the execution time but would still be faster than the first classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us finally test our classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9768339768339769, array([[1796.,   41.],\n",
       "        [   7.,  228.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, we train the second classifier with both training and validation sets\n",
    "hamMessages_train = X_intermediary[X_intermediary['type'] == 'ham'][\"text\"].tolist()\n",
    "spamMessages_train = X_intermediary[X_intermediary['type'] == 'spam'][\"text\"].tolist()\n",
    "\n",
    "cfl2.train2(hamMessages_train, spamMessages_train)\n",
    "\n",
    "messages_test = X_test[\"text\"].tolist()\n",
    "labels_test = X_test[\"type\"].tolist()\n",
    "\n",
    "cfl2.score(messages_test, labels_test)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
